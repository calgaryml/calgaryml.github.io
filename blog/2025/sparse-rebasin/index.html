<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry | Calgary Machine Learning Lab </title> <meta name="author" content=" "> <meta name="description" content="An exploration of why Lottery Ticket Hypothesis masks fail on new random initializations and how understanding weight symmetry in neural networks allows us to successfully reuse them."> <meta name="keywords" content="Calgary, ucalgary, University of Calgary, Alberta, u of c, machine learning, ml, neural networks, NN, deep learning, artificial intelligence, AI, lab, research lab, research group, yani ioannou, schulich, engineering"> <meta property="og:site_name" content="Calgary Machine Learning Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Calgary Machine Learning Lab | Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry"> <meta property="og:url" content="https://www.calgaryml.com/blog/2025/sparse-rebasin/"> <meta property="og:description" content="An exploration of why Lottery Ticket Hypothesis masks fail on new random initializations and how understanding weight symmetry in neural networks allows us to successfully reuse them."> <meta property="og:image" content="/assets/img/ucmllogo-og.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry"> <meta name="twitter:description" content="An exploration of why Lottery Ticket Hypothesis masks fail on new random initializations and how understanding weight symmetry in neural networks allows us to successfully reuse them."> <meta name="twitter:image" content="/assets/img/ucmllogo-og.png"> <meta name="twitter:site" content="@UCalgaryML"> <meta name="twitter:creator" content="@UCalgaryML"> <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ResearchOrganization",
      "name": "Calgary Machine Learning Lab",
      "alternateName": ["Calgary ML", "CML", "Calgary ML Lab"],
      "url": "https://www.calgaryml.com/",
      "logo": "https://www.calgaryml.com/assets/img/ucmllogo-text.svg",
      "description": "The Calgary Machine Learning Lab is a research group within the Schulich School of Engineering at the University of Calgary, focusing on Deep Neural Network (DNN) training, sparse networks, and efficient deep learning methods.",
      "foundingDate": "2021",
      "founder": {
        "@type": "Person",
        "name": "Yani Ioannou",
        "jobTitle": "Assistant Professor",
        "url": "https://www.yani.ai"
      },
      "parentOrganization": {
        "@type": "EducationalOrganization",
        "name": "University of Calgary",
        "url": "https://ucalgary.ca/"
      },
      "department": {
        "@type": "Organization",
        "name": "Schulich School of Engineering"
      },
      "address": {
        "@type": "PostalAddress",
        "streetAddress": "2500 University Drive NW",
        "addressLocality": "Calgary",
        "addressRegion": "AB",
        "postalCode": "T2N 1N4",
        "addressCountry": "CA"
      },
      "contactPoint": {
        "@type": "ContactPoint",
        "email": "yani.ioannou@ucalgary.ca",
        "contactType": "research inquiries",
        "availableLanguage": ["English"]
      },
      "sameAs": ["https://github.com/calgaryml", "https://twitter.com/UCalgaryML"],
      "knowsAbout": ["Machine Learning", "Deep Learning", "Computer Vision", "Sparse Neural Networks", "Artificial Intelligence"]
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://use.typekit.net/pzd6qvr.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.calgaryml.com/blog/2025/sparse-rebasin/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry",
            "description": "An exploration of why Lottery Ticket Hypothesis masks fail on new random initializations and how understanding weight symmetry in neural networks allows us to successfully reuse them.",
            "published": "July 14, 2025",
            "updatedDate": "November 01, 2025",
            "postAuthor": "Yani Ioannou",
            "authors": [
              
              {
                "author": "Mohammed Adnan",
                "authorURL": "https://www.linkedin.com/in/adnan-mohammed-/",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Rohan Jain",
                "authorURL": "https://www.linkedin.com/in/rohan-jain-6a1b2c3d/",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ekansh Sharma",
                "authorURL": "https://www.linkedin.com/in/ekansh-sharma-/",
                "affiliations": [
                  {
                    "name": "University of Toronto",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Rahul G. Krishnan",
                "authorURL": "https://www.cs.toronto.edu/~rahulgk/",
                "affiliations": [
                  {
                    "name": "University of Toronto",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yani Ioannou",
                "authorURL": "https://yani.ai/",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              }
              
            ],
            "doi": "10.48550/arXiv.2505.05143",
            "url": "https://proceedings.mlr.press/v267/adnan25a.html",
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img class="navbar" src="/assets/img/ucmllogo-text.svg" alt="Calgary Machine Learning Lab"> </a> <div class="navbar-brand social"> <a href="mailto:%79%61%6E%69.%69%6F%61%6E%6E%6F%75@%75%63%61%6C%67%61%72%79.%63%61" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9797-5888" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Qy9yv44AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/calgaryml" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://x.com/UCalgaryML" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/labmembers/">people </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/funding/">funding </a> </li> <li class="nav-item "> <a class="nav-link" href="/places/">calgary </a> </li> <li class="nav-item "> <a class="nav-link" href="/contactus/">contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry</h1> <p>An exploration of why Lottery Ticket Hypothesis masks fail on new random initializations and how understanding weight symmetry in neural networks allows us to successfully reuse them.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#tldr">TL;DR</a></li> <li class="toc-entry toc-h2"> <a href="#the-sparse-training-problem-and-the-lottery-ticket-hypothesis">The Sparse Training Problem and the Lottery Ticket Hypothesis</a> <ul> <li class="toc-entry toc-h3"><a href="#the-sparse-training-problem">The Sparse Training Problem</a></li> <li class="toc-entry toc-h3"><a href="#the-lottery-ticket-hypothesis">The Lottery Ticket Hypothesis</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#its-all-about-symmetry">It’s All About Symmetry</a> <ul> <li class="toc-entry toc-h3"> <a href="#the-geometry-of-the-sparse-training-problem">The Geometry of the Sparse Training Problem</a> <ul> <li class="toc-entry toc-h4"><a href="#dense-training-and-pruning">Dense Training and Pruning</a></li> <li class="toc-entry toc-h4"><a href="#the-lottery-ticket-hypothesis-1">The Lottery Ticket Hypothesis</a></li> <li class="toc-entry toc-h4"><a href="#the-sparse-training-problem-1">The Sparse Training Problem</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#the-hypothesis-a-tale-of-two-basins">The Hypothesis: A Tale of Two Basins</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#the-method-aligning-masks-with-permutations">The Method: Aligning Masks with Permutations</a> <ul> <li class="toc-entry toc-h3"><a href="#permutated-vs-the-lth-and-naive-baselines">Permutated vs. the LTH and Naive Baselines</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#the-result-it-works-approximately">The Result: It Works (Approximately!)</a> <ul> <li class="toc-entry toc-h3"><a href="#closing-the-performance-gap">Closing the Performance Gap</a></li> <li class="toc-entry toc-h3"><a href="#unlocking-diverse-solutions-with-a-single-mask">Unlocking Diverse Solutions with a Single Mask</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#key-insights-summarized">Key Insights Summarized</a></li> <li class="toc-entry toc-h2"><a href="#conclusion-and-future-directions">Conclusion and Future Directions</a></li> <li class="toc-entry toc-h2"><a href="#citing-our-work">Citing our work</a></li> </ul> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <p>The Lottery Ticket Hypothesis (LTH) demonstrates that remarkably sparse “winning ticket” neural network models can be trained to match the performance of their dense counterparts. However, there’s a catch: a winning ticket’s sparse mask is tightly coupled to the <em>original weight initialization</em> used to find it <d-cite key="frankle2019lth"></d-cite>. Using the same mask with any other random initialization results in a significant drop in performance — also known as the “sparse training problem”.</p> <p>Our <strong>ICML 2025</strong> paper “Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry” <d-cite key="adnan2025sparse"></d-cite> investigates the sparse training problem from a weight-space symmetry perspective and finds:</p> <ul> <li> <strong>The Problem is Misalignment:</strong> The reason LTH masks don’t generalize to new initializations is a misalignment of optimization basins in the loss landscape, which arises from the inherent geometry and permutation symmetries of neural networks. A mask found in one basin won’t work well if the new initialization starts in a different, symmetrically equivalent basin.</li> <li> <strong>The Solution is Alignment:</strong> We can approximate the permutation that aligns the basins of two different models<d-cite key="ainsworth2023git,jordan2023repair"></d-cite>, and applying this same permutation to the LTH mask, we can successfully train a sparse neural network from a <em>new</em> random initialization. The better the approximation of the permutation, the better the performance.</li> <li> <strong>Bridging the Performance Gap:</strong> Training with this <strong>permuted mask</strong> significantly improves generalization compared to naively using the original mask, nearly matching the performance of the original LTH solution across various models and datasets when the models are wide enough to allow accurate permutation matching.</li> <li> <strong>Unlocking Diversity:</strong> Unlike standard LTH, which consistently relearns the same solution <d-cite key="evci2022gradientflow"></d-cite>, our permutation-based method can train more diverse solutions when starting from different random initializations.</li> </ul> <hr> <h2 id="the-sparse-training-problem-and-the-lottery-ticket-hypothesis">The Sparse Training Problem and the Lottery Ticket Hypothesis</h2> <h3 id="the-sparse-training-problem">The Sparse Training Problem</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsetraining_pruned.svg" alt="Diagram showing standard training and pruning with a dense model." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 1(a): The standard dense training and pruning pipeline creates a good pruned solution.</div> </div> <p>Dense neural network training followed by pruning is a well-established method for obtaining smaller, efficient models for inference, we can train a dense neural network to convergence, and then prune it to obtain a sparse mask. The resulting sparse model can often match the performance of the original dense model <d-cite key="han2015deep,frankle2019lth"></d-cite>.</p> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsetraining_sparsetraining.svg" alt="Diagram showing the sparse training problem." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 1(b): The sparse training problem: applying the mask from the pruned solution to a new, different random initialization results in poor performance.</div> </div> <p>If we can use a sparse neural network at test/inference time, why can’t we train the model sparse from the beginning? The sparse training problem arises when we try to reuse the sparse mask obtained from pruning to train a new model from a different random initialization. Naively applying the mask to this new initialization leads to a significant drop in performance. This phenomenon has been observed across various architectures and datasets <d-cite key="frankle2019lth"></d-cite>.</p> <h3 id="the-lottery-ticket-hypothesis">The Lottery Ticket Hypothesis</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsetraining_lth_revised.svg" alt="Diagram showing the original Lottery Ticket Hypothesis." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 1(c): The Lottery Ticket Hypothesis (LTH) proposes a solution to the sparse training problem by rewinding the weights of the remaining connections to their values from very early in training before retraining the sparse model. Unfortunately it does not address the issue of applying the mask to a new initialization, and has been shown to effectively relearn the same solution as the original dense model.</div> </div> <p>The Lottery Ticket Hypothesis (LTH) <d-cite key="frankle2019lth"></d-cite> proposes a solution to the sparse training problem. It suggests we can re-use a sparse mask by rewinding the weights of the remaining connections to their values from very early in training before retraining the sparse model. The standard LTH methodology is:</p> <ol> <li>Train a full, dense neural network.</li> <li>Prune the connections with the smallest magnitude weights to get a sparse mask.</li> <li>“Rewind” the weights of the remaining connections to their values from very early in training and train the sparse neural network again.</li> </ol> <p>This process can produce sparse models that match the performance of the original dense one <d-cite key="frankle2019lth"></d-cite>, however requires expensive dense pre-training from many early training checkpoints in practice to identify a “winning ticket”. Rather than the sparse training problem being solved, i.e. <strong>re-using a mask with a new random initialization</strong>, the LTH sidesteps it by ensuring the mask is trained within the original solution basin<d-cite key="evci2022gradientflow"></d-cite>. In fact, it has been shown that the LTH does not learn new solutions, but rather relearns the same solution as the original dense model <d-cite key="evci2022gradientflow"></d-cite>.</p> <p>The motivation of the <strong>sparse training problem</strong> is to answer: <strong>What if we could just use a winning ticket mask to train a sparse model from a <em>new</em> random initialization?</strong> Unfortunately, the LTH doesn’t solve this, but does point the way in that it highlights the importance of the <strong>coupling between the sparse mask and the original weight initialization</strong>.</p> <h2 id="its-all-about-symmetry">It’s All About Symmetry</h2> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-10 mt-3 mt-md-0"> <img src="/assets/img/sparse-rebasin/weightsymmetry3.svg" alt="Diagram illustrating that swapping two neurons results in a functionally identical network." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 2: Swapping two neurons (and their incoming and outgoing weights) results in a functionally identical network, illustrating permutation or weight symmetry.</div> </div> <p>The answer lies in a fundamental property of neural networks: permutation or <strong>weight symmetry</strong>. If you take a layer in a neural network model and swap two of its neurons — including their incoming and outgoing weights — the function the neural network represents remains identical <d-cite key="nielsen1990,entezari2022role"></d-cite>. However, in the high-dimensional space of weight space where we optimize, these two neural network models are at completely different locations.</p> <p>This symmetry means the loss landscape is filled with many identical, mirrored <strong>loss basins</strong>. When we train a model from a random initialization, it descends into one of these basins. Recent work suggests that most solutions found by SGD lie in a <em>single</em> basin, once you account for these permutations <d-cite key="entezari2022role,ainsworth2023git"></d-cite>.</p> <h3 id="the-geometry-of-the-sparse-training-problem">The Geometry of the Sparse Training Problem</h3> <h4 id="dense-training-and-pruning">Dense Training and Pruning</h4> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsebasin_densepruning.svg" alt="Loss landscape showing dense training and weight magnitude-based pruning." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 3(a): Dense neural network training and pruning; a dense neural network model of only two neurons, each with a single weight $w_0$ and $w_1$ respectively can illustrate the geometry of loss landscapes, and the sparse training problem. Here, dense neural network training and weight magnitude-based pruning results in performant neural network for inference with a sparse mask $\mathbf{m}_A$.</div> </div> <p>Here we show the loss landscape of a neural network model of only two neurons, each with a single weight $w_0$ and $w_1$ respectively. The model has two symmetric loss basins/local minima, corresponding to the two possible permutations of the neurons. This simple model can illustrate the geometry of loss landscapes, and convey our intuition about the sparse training problem.</p> <p>In Figure 3(a) a neural network $A$ is trained from random initialization $\mathbf{w}^{t=0}_A$ to a good solution $\mathbf{w}^{t=T}_A$, and pruned to remove the smallest magnitude weight, defining a mask $\mathbf{m}_A$ and sparse neural network model $\mathbf{w}^{t=T}_A \odot m_A$. In general such dense training and pruning works well, and maintains good generalization (e.g. test accuracy).</p> <h4 id="the-lottery-ticket-hypothesis-1">The Lottery Ticket Hypothesis</h4> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsebasin_lth.svg" alt="Loss landscape showing Lottery Ticket Hypothesis methodology." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 3(b): The Lottery Ticket Hypothesis (LTH) suggests the sparse mask $\mathbf{m}_A$ can be trained using the same training procedure as the original model, but with the mask applied from almost the start, achieving sparse training.</div> </div> <p>In Figure 3(b) we again train neural network $A$ from the same random initialization $\mathbf{w}^{t=0}_A$ however, in this case we train sparse, i.e. using the mask $\mathbf{m}_A$. This is the equivalent of projecting our initial weights down to the subspace defined by the mask, in this case the single dimension $\mathbf{w}_0$, and training in that restricted subspace, i.e. in this case along the one-dimensional subspace aligned with $\mathbf{w}_0$. We still find a good solution $\mathbf{w}^{t=T}_A \odot \mathbf{m}_A$ which maintains good generalization (e.g. test accuracy).</p> <p>This is what the Lottery Ticket Hypothesis (LTH) suggests: that the sparse mask $\mathbf{m}_A$ can be trained using the same training procedure as the original model, but with the mask applied from almost the start, achieving sparse training.</p> <h4 id="the-sparse-training-problem-1">The Sparse Training Problem</h4> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsebasin_sparsetrainingproblem.svg" alt="Loss landscape showing dense training and weight magnitude-based pruning." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 3(c): The sparse training problem is illustrated by attempting to train model $B$ from a new random initialization, $\mathbf{w}^{t=0}_B$, while re-using the sparse mask $m_A$ discovered from pruning. Sparse training of model $B$ fails with the original mask, as one of the most important weights is not preserved.</div> </div> <p>Finally, in Figure 3(c) we illustrate the sparse training problem. Here we attempt to train a new neural network $B$ from a new random initialization, $\mathbf{w}^{t=0}_B$, while re-using the sparse mask $\mathbf{m}_A$ discovered from pruning neural network $A$. Sparse training of neural network $B$ fails with the original mask, as one of the most important weights is not preserved when we project using $\mathbf{m}_A$, projecting our weight instead to a location far from the solution basin, and leading to poor generalization (e.g. test accuracy).</p> <h3 id="the-hypothesis-a-tale-of-two-basins">The Hypothesis: A Tale of Two Basins</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/sparsebasin_permuted.svg" alt="Loss landscape showing dense training and weight magnitude-based pruning." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 3(d): Our solution is to permute the mask to $\pi(m_A)$, which aligns with model B's basin and enables successful sparse training (green path).The permuted mask $\pi(m_A)$ aligns with model B's basin, enabling successful sparse training (green path).</div> </div> <p>This brings us to our core hypothesis <d-cite key="adnan2025sparse"></d-cite>: <strong>An LTH mask fails on a new initialization because the mask is aligned to one basin, while the new random initialization has landed in another.</strong></p> <p>The optimization process is essentially starting in the wrong valley for the map it’s been given. Naively applying the mask pulls the new initialization far away from a good solution path, leading to poor performance <d-cite key="adnan2025sparse"></d-cite>.</p> <p><strong>But what if we could “rotate” the mask to match the orientation of the new basin?</strong> This is exactly what we propose.</p> <hr> <h2 id="the-method-aligning-masks-with-permutations">The Method: Aligning Masks with Permutations</h2> <p>Our method leverages recent advances in model merging, like Git Re-Basin <d-cite key="ainsworth2023git"></d-cite>, which find the permutation that aligns the neurons of two separately trained models. Our training paradigm is as follows <d-cite key="adnan2025sparse"></d-cite>:</p> <ol> <li> <strong>Train Two Dense Models:</strong> Start with two different random initializations, $\mathbf{w}_A^{t=0}$ and $\mathbf{w}_B^{t=0}$, and train them to convergence to get two dense models, $\mathbf{w}_A^{t=T}$ and $\mathbf{w}_B^{t=T}$, or <code class="language-plaintext highlighter-rouge">Model A</code> and <code class="language-plaintext highlighter-rouge">Model B</code>. <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/method_step1.svg" alt="Method Step 1: Dense Training of Two models." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> </div> </li> <li> <strong>Get the LTH Mask:</strong> Prune <code class="language-plaintext highlighter-rouge">Model A</code> using standard iterative magnitude pruning (IMP) to get a sparse “winning ticket” mask, $\mathbf{m}_A$. <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/method_step2.svg" alt="Method Step 1: Dense Training of Two models." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> </div> </li> <li> <strong>Find the Permutation relating the Models:</strong> Use an <strong>activation matching</strong> algorithm <d-cite key="jordan2023repair"></d-cite> to find the permutation, $\pi$, that best aligns the neurons of <code class="language-plaintext highlighter-rouge">Model A</code> with <code class="language-plaintext highlighter-rouge">Model B</code>, i.e. $\mathbf{w}_B^{t=T} = \pi(\mathbf{w}_A^{t=T})$. This essentially finds the “rotation” or permutation needed to map one solution basin onto the other. <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/method_step3.svg" alt="Method Step 1: Dense Training of Two models." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> </div> </li> <li> <strong>Permute the Mask:</strong> Apply the permutation $\pi$ to the mask $\pi(\mathbf{m}_A)$ for <code class="language-plaintext highlighter-rouge">Model A</code> to get a new, aligned mask: $\mathbf{m}_B = \pi(\mathbf{m}_A)$ for <code class="language-plaintext highlighter-rouge">Model B</code>. <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/method_step4.svg" alt="Method Step 1: Dense Training of Two models." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> </div> </li> <li> <strong>Train from Scratch (Almost)!</strong> Train a new sparse model starting from the $\mathbf{w}_B$ initialization (rewound to an early checkpoint, $k$), but using the <strong>permuted mask</strong> $\pi(\mathbf{m}_A)$.</li> </ol> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/method_step5.svg" alt="Method Step 1: Dense Training of Two models." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> </div> <h3 id="permutated-vs-the-lth-and-naive-baselines">Permutated vs. the LTH and Naive Baselines</h3> <div class="container l-screen"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/methodology.svg" alt="Diagram of the training paradigm, from training dense models to permutation matching and final sparse training." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 4: The overall framework of our training procedure <d-cite key="adnan2025sparse"></d-cite>. We use two trained dense models to find a permutation $\pi$. This permutation is then applied to the mask from Model A, allowing it to be successfully used to train Model B from a random initialization.</div> </div> <p>We present the methodology of the three different training paradigms we compare in our results here in Figure 4:</p> <ol> <li> <strong>LTH</strong>: The original Lottery Ticket Hypothesis approach, which trains a dense model and then prunes it.</li> <li> <strong>Naive</strong>: A straightforward application of the pruned mask from Model A to Model B without any permutation, this is the standard sparse training problem setup, and performs poorly.</li> <li> <strong>Permuted</strong>: Our proposed method, which finds a permutation of the weights to better align the two models.</li> </ol> <hr> <h2 id="the-result-it-works-approximately">The Result: It Works (Approximately!)</h2> <p>Across a wide range of experiments, our method demonstrates that aligning the mask is the key to solving the sparse training problem for LTH masks <d-cite key="adnan2025sparse"></d-cite>. Of course the permutation matching is only approximate, and so the performance doesn’t perfectly match the original LTH solution, but it comes remarkably close, especially as model width increases which has been shown to improve permutation matching quality <d-cite key="ainsworth2023git,jordan2023repair"></d-cite>.</p> <h3 id="closing-the-performance-gap">Closing the Performance Gap</h3> <div class="container xl-screen"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/results_cifar10.svg" alt="Graphs showing test accuracy vs rewind points for ResNet20 on CIFAR-10 at different sparsity levels and widths." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 5: Test accuracy on CIFAR-10 for ResNet20 of varying widths (`w`) and sparsities. The permuted solution (blue) consistently outperforms the naive one (orange) and gets closer to the LTH baseline (green), especially as model width increases <d-cite key="adnan2025sparse"></d-cite>.</div> </div> <div class="container xl-screen"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/results_cifar100.svg" alt="Graphs showing test accuracy vs rewind points for ResNet20 on CIFAR-100 at different sparsity levels and widths." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 6: Test accuracy on CIFAR-100 for ResNet20 of varying widths (`w`) and sparsities. The permuted solution (blue) consistently outperforms the naive one (orange) and gets closer to the LTH baseline (green), especially as model width increases <d-cite key="adnan2025sparse"></d-cite>.</div> </div> <div class="container xl-screen"> <div class="row"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/sparse-rebasin/results_vgg.svg" alt="Graphs showing test accuracy vs rewind points for VGG-11 on CIFAR-10 at different sparsity levels and widths." class="img-fluid rounded z-depth-0 mx-auto d-block" loading="eager"> </div> </div> <div class="caption">Figure 7: Test accuracy on CIFAR-10 for VGG-11 of varying sparsities. The permuted solution (blue) consistently outperforms the naive one (orange) and gets closer to the LTH baseline (green), especially as model width increases <d-cite key="adnan2025sparse"></d-cite>.</div> </div> <p>When we compare the performance of the standard <code class="language-plaintext highlighter-rouge">LTH</code> solution, the <code class="language-plaintext highlighter-rouge">Naive</code> solution (un-permuted mask on a new init), and our <code class="language-plaintext highlighter-rouge">Permuted</code> solution, the results are clear. The <code class="language-plaintext highlighter-rouge">Permuted</code> approach consistently and significantly outperforms the <code class="language-plaintext highlighter-rouge">Naive</code> baseline, closing most of the performance gap to the original <code class="language-plaintext highlighter-rouge">LTH</code> solution. The effect is especially pronounced for wider models and higher sparsity levels, where the <code class="language-plaintext highlighter-rouge">Naive</code> method struggles most.</p> <h3 id="unlocking-diverse-solutions-with-a-single-mask">Unlocking Diverse Solutions with a Single Mask</h3> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table 1: Ensemble Diversity Metrics for CIFAR-10/CIFAR-100: Although the mean test accuracy of LTH is higher, the ensemble of permuted models achieves better test accuracy due to better functional diversity of permuted models. Here we compare several measurements of function space similarity between the models including disagreement, which measures prediction differences [citation], and Kullback–Leibler (KL)/Jenson-Shannon (JS) divergence, which quantify how much the output distributions of different models differ [citation]. As shown, the permuted masks achieve similar diversity as computational expensive IMP solutions, also resulting in ensembles with a similar increase in generalization. </div> <table> <thead> <tr> <th style="text-align: left">Mask</th> <th style="text-align: left">Test Accuracy (%)</th> <th style="text-align: left">Ensemble Acc. (%)</th> <th style="text-align: left">Disagreement</th> <th style="text-align: left">KL</th> <th style="text-align: left">JS</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>ResNet20x{1}/CIFAR-10</strong></td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">none (dense)</td> <td style="text-align: left">92.76 ± 0.106</td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> </tr> <tr> <td style="text-align: left">IMP</td> <td style="text-align: left">91.09 ± 0.041</td> <td style="text-align: left">93.25</td> <td style="text-align: left">0.093</td> <td style="text-align: left">0.352</td> <td style="text-align: left">0.130</td> </tr> <tr> <td style="text-align: left">LTH</td> <td style="text-align: left"><strong>91.15 ± 0.163</strong></td> <td style="text-align: left">91.43</td> <td style="text-align: left">0.035</td> <td style="text-align: left">0.038</td> <td style="text-align: left">0.011</td> </tr> <tr> <td style="text-align: left">permuted</td> <td style="text-align: left">89.38 ± 0.170</td> <td style="text-align: left"><strong>91.75</strong></td> <td style="text-align: left">0.107</td> <td style="text-align: left"><strong>0.273</strong></td> <td style="text-align: left"><strong>0.091</strong></td> </tr> <tr> <td style="text-align: left">naive</td> <td style="text-align: left">88.68 ± 0.205</td> <td style="text-align: left">91.07</td> <td style="text-align: left"><strong>0.113</strong></td> <td style="text-align: left">0.271</td> <td style="text-align: left">0.089</td> </tr> <tr> <td style="text-align: left"><strong>ResNet20x{4}/CIFAR-100</strong></td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">none (dense)</td> <td style="text-align: left">78.37 ± 0.059</td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> <td style="text-align: left">-</td> </tr> <tr> <td style="text-align: left">IMP</td> <td style="text-align: left">74.46 ± 0.321</td> <td style="text-align: left">79.27</td> <td style="text-align: left">0.259</td> <td style="text-align: left">1.005</td> <td style="text-align: left">0.372</td> </tr> <tr> <td style="text-align: left">LTH</td> <td style="text-align: left"><strong>75.35 ± 0.204</strong></td> <td style="text-align: left">75.99</td> <td style="text-align: left">0.117</td> <td style="text-align: left">0.134</td> <td style="text-align: left">0.038</td> </tr> <tr> <td style="text-align: left">permuted</td> <td style="text-align: left">72.48 ± 0.356</td> <td style="text-align: left"><strong>77.85</strong></td> <td style="text-align: left">0.278</td> <td style="text-align: left">0.918</td> <td style="text-align: left">0.327</td> </tr> <tr> <td style="text-align: left">naive</td> <td style="text-align: left">71.05 ± 0.366</td> <td style="text-align: left">76.15</td> <td style="text-align: left"><strong>0.290</strong></td> <td style="text-align: left"><strong>0.970</strong></td> <td style="text-align: left"><strong>0.348</strong></td> </tr> </tbody> </table> </div> <p>A limitation of LTH is that it consistently converges to very similar solutions to the original pruned model <d-cite key="evci2022gradientflow"></d-cite>, and it is concluded that this occurs because the LTH is always trained with the same initialization/rewind point, and effectively relearns the same solution. Our hypothesis is that permuted LTH masks, trained with distinct initialization/rewind points and subject to approximation errors in permutation matching, may learn more diverse functions than the LTH itself.</p> <p>We analyze the diversity of sparse models trained at 90% sparsity, with either a permuted mask (permuted), the LTH mask (naive), LTH mask &amp; init. and the original pruned solution (IMP) on which the LTH is based. We follow the same analysis as <d-cite key="evci2022gradientflow"></d-cite> and compare the diversity of the resulting models, over five different training runs, using disagreement score, KL divergence and JS divergence. We also compare with an ensemble of five models trained independently with different random seeds. As shown in Table 1, an ensemble of permuted models shows higher diversity across all the metrics than the LTH, showing that the permuted models learn a more diverse set of solutions. We provide additional details in the appendix.</p> <hr> <h2 id="key-insights-summarized">Key Insights Summarized</h2> <p>This investigation into aligning sparse masks reveals:</p> <ol> <li> <strong>Symmetry is the Culprit:</strong> The failure of LTH masks to transfer to new initializations is not a fundamental flaw of sparsity itself, but a consequence of re-using a mask naively, and the resulting misalignment of the optimization basins corresponding to a new initialization and the mask’s original basin.</li> <li> <strong>Permutation is the Solution:</strong> By explicitly finding and correcting for this misalignment—by permuting the sparse mask we can successfully reuse a sparse mask derived from pruning to train a high-performing sparse model from a completely new random initialization.</li> <li> <strong>Diversity is a Feature:</strong> This approach not only solves a practical problem with LTH but also opens the door to finding more functionally diverse sparse solutions than the LTH alone.</li> <li> <strong>Wider Models Align Better:</strong> The effectiveness of the permutation alignment increases with model width and sparsity, suggesting that wider models provide a richer structure for permutation matching to work effectively, and highlighting the interplay between model architecture and optimization geometry.</li> </ol> <h2 id="conclusion-and-future-directions">Conclusion and Future Directions</h2> <p>This work provides a new lens through which to view the sparse training problem. The success of a lottery ticket isn’t just about the mask’s structure, but also its <em>alignment</em> with the optimization landscape. While our method requires training two dense models to find the permutation, we also found that a models could be matched earlier in training. Regardless, our analysis is thus a tool for insight rather than efficiency, it proves a crucial point: it is possible to better align lottery ticket masks with new initializations.</p> <p>This opens up exciting new questions. By showing that the barriers of sparse training can be overcome by understanding its underlying geometry, we hope to spur future work that makes training sparse models from scratch a practical and powerful reality.</p> <hr> <h2 id="citing-our-work">Citing our work</h2> <p>If you find this work useful, please consider citing it using the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">adnan2025sparse</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Adnan, Mohammed and Jain, Rohan and Sharma, Ekansh and Krishnan, Rahul G. and Ioannou, Yani}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Forty-Second International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">venue</span> <span class="p">=</span> <span class="s">{Vienna, Austria}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-07-13-sparse-rebasin.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container territorial-acknowledgement pb-2"> We would like to acknowledge the traditional territories of the People of the Treaty 7 region in Southern Alberta which includes the Blackfoot Confederacy (comprising the Siksika, Piikani, and Kainai First Nations), as well as the Tsuut’ina First Nation, and the Stoney Nakoda (including the Chiniki, Bearspaw, and Wesley First Nations). The City of Calgary is also home to Métis Nation of Alberta, Region 3. </div> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/calgaryml/calgaryml.github.io" rel="external nofollow noopener" target="_blank">customized theme</a> based on <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos © Yani Ioannou. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T1LVR8501M"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T1LVR8501M");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>
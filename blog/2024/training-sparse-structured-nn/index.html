<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dynamic Sparse Training with Structured Sparsity | Calgary Machine Learning Lab </title> <meta name="author" content=" "> <meta name="description" content="Learning Performant and Efficient Representations suitable for Hardware Acceleration"> <meta name="keywords" content="Calgary, ucalgary, University of Calgary, Alberta, u of c, machine learning, ml, neural networks, NN, deep learning, artificial intelligence, AI, lab, research lab, research group, yani ioannou, schulich, engineering"> <meta property="og:site_name" content="Calgary Machine Learning Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Calgary Machine Learning Lab | Dynamic Sparse Training with Structured Sparsity"> <meta property="og:url" content="https://www.calgaryml.com/blog/2024/training-sparse-structured-nn/"> <meta property="og:description" content="Learning Performant and Efficient Representations suitable for Hardware Acceleration"> <meta property="og:image" content="/assets/img/ucmllogo-og.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Dynamic Sparse Training with Structured Sparsity"> <meta name="twitter:description" content="Learning Performant and Efficient Representations suitable for Hardware Acceleration"> <meta name="twitter:image" content="/assets/img/ucmllogo-og.png"> <meta name="twitter:site" content="@UCalgaryML"> <meta name="twitter:creator" content="@UCalgaryML"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": " "
        },
        "url": "https://www.calgaryml.com/blog/2024/training-sparse-structured-nn/",
        "@type": "BlogPosting",
        "description": "Learning Performant and Efficient Representations suitable for Hardware Acceleration",
        "headline": "Dynamic Sparse Training with Structured Sparsity",
        
        "sameAs": ["https://orcid.org/0000-0002-9797-5888", "https://scholar.google.com/citations?user=Qy9yv44AAAAJ", "https://github.com/calgaryml", "https://twitter.com/UCalgaryML"],
        
        "name": " ",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://use.typekit.net/pzd6qvr.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.calgaryml.com/blog/2024/training-sparse-structured-nn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.MJX-TEX,.MJX-TEX *{font-family:"Times New Roman"}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Dynamic Sparse Training with Structured Sparsity",
            "description": "Learning Performant and Efficient Representations suitable for Hardware Acceleration",
            "published": "May 07, 2024",
            "authors": [
              
              {
                "author": "Mike Lasby",
                "authorURL": "https://github.com/mklasby",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anna Golubeva",
                "authorURL": "https://annagolubeva.github.io",
                "affiliations": [
                  {
                    "name": "MIT, IAIFI",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Utku Evci",
                "authorURL": "https://utkuevci.com",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Mihai Nica",
                "authorURL": "https://nicam.uoguelph.ca",
                "affiliations": [
                  {
                    "name": "University of Guelph, Vector Institute for AI",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yani Ioannou",
                "authorURL": "https://yani.ai",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              }
              
            ],
            "doi": "10.48550/arXiv.2305.02299",
            "url": "https://openreview.net/pdf?id=kOBkxFRKTA",
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img class="navbar" src="/assets/img/ucmllogo-text.svg" alt="Calgary Machine Learning Lab"> </a> <div class="navbar-brand social"> <a href="mailto:%79%61%6E%69.%69%6F%61%6E%6E%6F%75@%75%63%61%6C%67%61%72%79.%63%61" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9797-5888" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Qy9yv44AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/calgaryml" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://x.com/UCalgaryML" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/labmembers/">people </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/funding/">funding </a> </li> <li class="nav-item "> <a class="nav-link" href="/places/">calgary </a> </li> <li class="nav-item "> <a class="nav-link" href="/contactus/">contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Dynamic Sparse Training with Structured Sparsity</h1> <p>Learning Performant and Efficient Representations suitable for Hardware Acceleration</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#tldr">TL;DR</a></li> <li class="toc-entry toc-h2"> <a href="#the-quest-for-more-efficient-neural-networks">The Quest for More Efficient Neural Networks</a> <ul> <li class="toc-entry toc-h3"><a href="#why-sparse-neural-networks">Why Sparse Neural Networks?</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#paths-to-sparsity-pruning-lottery-tickets-and-dynamic-sparse-training">Paths to Sparsity: Pruning, Lottery Tickets, and Dynamic Sparse Training</a> <ul> <li class="toc-entry toc-h3"><a href="#traditional-pruning-effective-but-costly">Traditional Pruning: Effective but Costly</a></li> <li class="toc-entry toc-h3"><a href="#the-sparse-training-problem-training-with-a-fixed-sparse-mask">The Sparse Training Problem: Training with a Fixed Sparse Mask</a></li> <li class="toc-entry toc-h3"><a href="#dynamic-sparse-training-dst-training-sparse-from-the-start">Dynamic Sparse Training (DST): Training Sparse from the Start</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#unstructured-vs-structured-sparsity">Unstructured vs. Structured Sparsity</a> <ul> <li class="toc-entry toc-h3"><a href="#unstructured-sparsity">Unstructured Sparsity</a></li> <li class="toc-entry toc-h3"><a href="#structured-sparsity-eg-removing-neuronsblocks">Structured Sparsity (e.g., removing neurons/blocks)</a></li> <li class="toc-entry toc-h3"><a href="#nm-fine-grained-structured-sparsity">N:M Fine-grained Structured Sparsity</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#dynamic-sparse-training-for-learning-structured-sparse-representations">Dynamic Sparse Training for Learning Structured Sparse Representations</a> <ul> <li class="toc-entry toc-h3"><a href="#constant-fan-in-sparsity">Constant Fan-in Sparsity</a></li> <li class="toc-entry toc-h3"><a href="#the-hidden-trick-of-dst-neuron-ablation">The Hidden Trick of DST: Neuron Ablation</a></li> <li class="toc-entry toc-h3"><a href="#the-srigl-algorithm">The SRigL Algorithm</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#key-results-performance-and-acceleration">Key Results: Performance and Acceleration</a> <ul> <li class="toc-entry toc-h3"><a href="#matching-dense-accuracy-with-structured-sparsity">Matching Dense Accuracy with Structured Sparsity</a></li> <li class="toc-entry toc-h3"><a href="#the-importance-of-neuron-ablation">The Importance of Neuron Ablation</a></li> <li class="toc-entry toc-h3"><a href="#real-world-speedups">Real-World Speedups</a></li> <li class="toc-entry toc-h3"><a href="#not-only-efficiency-srigl-enables-new-applications-of-neural-networks">Not Only Efficiency: SRigL Enables New Applications of Neural Networks</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#conclusion-and-future-horizons">Conclusion and Future Horizons</a></li> </ul> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <p>The challenge in training sparse neural networks is to achieve both high accuracy and practical hardware acceleration. Unstructured sparsity often yields good performance but is hard to speed up, while traditional structured sparsity can hurt performance.</p> <p>Our International Conference in Learning Representations (ICLR) 2024 paper, <a href="https://openreview.net/forum?id=kOBkxFRKTA" rel="external nofollow noopener" target="_blank">“Dynamic Sparse Training with Structured Sparsity”</a> <d-cite key="Lasby2024SRigL"></d-cite> introduces Structured RigL (SRigL) to addresses this by dynamically learning hardware-friendly sparse weight representations without sacrificing accuracy. Key findings of the work include:</p> <ul> <li>SRigL successfully learns a combination of fine-grained N:M structured sparsity (constant fan-in) and neuron-level sparsity (neuron ablation) dynamically from a sparse initialization.</li> <li>The explicit integration of neuron ablation, a behavior implicitly learned by unstructured DST methods at high sparsities, is crucial for SRigL to match the generalization performance of dense and unstructured sparse models, even at extreme sparsities (up to 99%).</li> <li>The learned structured sparsity enables a “condensed sparse representation,” which translates to significant real-world inference speedups on commodity CPUs (up to $3.4 \times$ vs. dense at 90% sparsity) and GPUs (up to $1.7 \times$ vs. dense at 90% sparsity), outperforming unstructured sparse formats in many practical scenarios.</li> <li>SRigL demonstrates a viable path to train sparse neural networks that are both highly accurate and practically efficient, bridging the gap between unstructured DST performance and structured sparsity acceleration.</li> </ul> <h2 id="the-quest-for-more-efficient-neural-networks">The Quest for More Efficient Neural Networks</h2> <p>State-of-the-art deep neural networks (DNNs) have achieved remarkable feats, but their ever-increasing size brings ballooning training costs, often outstripping Moore’s Law. This trend makes cutting-edge AI research less accessible. While techniques exist to prune trained dense models, effectively reducing their parameter count by 85-95% without sacrificing generalization, they relay on dense pre-training. Can we train these sparse, efficient networks without dense training?</p> <h3 id="why-sparse-neural-networks">Why Sparse Neural Networks?</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-8 mt-3 mt-md-0"> Sparse neural networks offer several compelling advantages: <ul> <li>For a fixed number of weights, they can offer better generalization and fewer Floating Point Operations (FLOPs) at inference.</li> <li>They hold the potential to significantly reduce the computational cost of training.</li> <li>They provide a way to learn the inherent structure of neural networks, moving beyond a "one-size-fits-all" dense architecture.</li> </ul> </div> <div class="col-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsenn_simple.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsenn_simple.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Sparse Neural Network." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <h2 id="paths-to-sparsity-pruning-lottery-tickets-and-dynamic-sparse-training">Paths to Sparsity: Pruning, Lottery Tickets, and Dynamic Sparse Training</h2> <p>Several approaches have been developed to tackle the challenge of obtaining sparse networks, these include:</p> <h3 id="traditional-pruning-effective-but-costly">Traditional Pruning: Effective but Costly</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_pruning.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_pruning.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Pruning is highly effective, but requires dense pre-training." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Pruning Dense Trained Models: Pruning a trained model is highly effective, but requires pre-trained models.</div> </div> <p>Standard pruning techniques involve training a full, dense network and then removing (pruning) weights deemed less important, typically those with the smallest magnitude. This can be done once (“one-shot pruning”) or iteratively. While effective at finding highly sparse subnetworks that retain accuracy, this still necessitates the expensive initial dense training.</p> <h3 id="the-sparse-training-problem-training-with-a-fixed-sparse-mask">The Sparse Training Problem: Training with a Fixed Sparse Mask</h3> <div class="container"> <div class="row align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_problem.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_problem.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="The sparse training problem." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Sparse Training Problem: Training a sparse mask from random initialization doesn't recover the generalization of a pruned dense model, even for a known good mask.</div> </div> <p>Despite the success of pruning, simply training a sparse network from a random initialization, even with a known “good” sparse mask (the pattern of zeroed-out weights), often leads to poor performance compared to its dense counterpart or a pruned dense model. This is known as the sparse training problem.</p> <p>The Lottery Ticket Hypothesis (LTH) <d-cite key="Frankle2019LTH"></d-cite> proposed that within a large, randomly initialized dense network, there exist smaller subnetworks (the “winning tickets”) that, when trained in isolation from their original initialization weights (or weights from very early in training <d-cite key="Frankle2020LinearMode"></d-cite>), can achieve accuracy comparable to the full dense network. This was a foundational idea, suggesting that specific initializations within a sparse structure are crucial. However, finding these “winning tickets” is computationally expensive, especially for larger models, and the original findings were primarily on smaller datasets <d-cite key="Liu2019RethinkingPruning"></d-cite>. Research further showed that these Lottery Tickets often end up re-learning the solution that would have been found by pruning the dense model they originated from <d-cite key="Evci2022GradientFlow"></d-cite>.</p> <h3 id="dynamic-sparse-training-dst-training-sparse-from-the-start">Dynamic Sparse Training (DST): Training Sparse from the Start</h3> <div class="container"> <div class="row align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_dst.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_dst.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Dynamic Sparse Training (DST) methods are an alternative sparse training method that work well." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Dynamic Sparse Training (DST) methods are an alternative sparse training, where the mask is changed over the course of training. DST methods can match the generalization performance of standard dense training.</div> </div> <p>Dynamic Sparse Training (DST) methods offer a more direct approach to training sparse networks. Techniques like Sparse Evolutionary Training (SET) <d-cite key="Mocanu2018SET"></d-cite> and Rigging the Lottery Ticket (RigL) <d-cite key="Evci2020RigL"></d-cite> train networks that are sparse from initialization to the final solution (“sparse-to-sparse”). They achieve this by dynamically changing the sparse connectivity during training: periodically pruning less salient connections (e.g., small magnitude weights) and growing new ones (e.g., where the gradient magnitude is large). DST can achieve generalization comparable to dense training at high sparsity levels.</p> <h2 id="unstructured-vs-structured-sparsity">Unstructured vs. Structured Sparsity</h2> <div class="container"> <div class="row align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_unstructuredvsstructured.svg" sizes="95vw"></source> <img src="/assets/img/srigl_unstructuredvsstructured.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Unstructured vs. Structured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Unstructured vs. Structured sparsity. Unstructured sparsity is difficult to use efficiently on current computational hardware, such as CPUs and GPUs.</div> </div> <h3 id="unstructured-sparsity">Unstructured Sparsity</h3> <p>A significant challenge with many DST methods like RigL is that they typically produce <strong>unstructured sparsity</strong>. This means individual weights are zeroed out irregularly across the weight matrices.</p> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_unstructuredwmatrix.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_unstructuredwmatrix.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Untructured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Unstructured Sparsity. A neural network with unstructured sparse weights, and the corresponding weight matrix.</div> </div> <ul> <li> <strong>Pros:</strong> Can achieve excellent generalization at very high sparsities (85-95%); fewer theoretical FLOPs.</li> <li> <strong>Cons:</strong> Poorly supported by standard hardware (CPUs/GPUs) and acceleration libraries, meaning theoretical speedups often don’t translate into real-world gains.</li> </ul> <h3 id="structured-sparsity-eg-removing-neuronsblocks">Structured Sparsity (e.g., removing neurons/blocks)</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_structuredwmatrix.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_structuredwmatrix.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Structured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Structured Sparsity. A neural network with structured sparse weights, and the corresponding weight matrix.</div> </div> <p>In contrast, <strong>structured sparsity</strong> involves removing entire blocks of weights, such as channels, filters, or even neurons.</p> <ul> <li> <strong>Pros:</strong> Much better hardware support, leading to practical speedups as it often results in effectively smaller dense operations.</li> <li> <strong>Cons:</strong> Often leads to poorer generalization compared to unstructured sparsity at the same overall sparsity level, as it’s a coarser form of pruning.</li> </ul> <h3 id="nm-fine-grained-structured-sparsity">N:M Fine-grained Structured Sparsity</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_nmwmatrix.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_nmwmatrix.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="N:M Structured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: N:M Structured Sparsity. A neural network with N:M structured sparse weights, and the corresponding weight matrix.</div> </div> <p>N:M fine-grained sparsity is a compromise where, within small contiguous blocks of M weights, exactly N weights are non-zero. NVIDIA’s Ampere GPUs support 2:4 sparsity, offering some acceleration <d-cite key="Mishra2021Accelerating,Nvidia2020Ampere"></d-cite>.</p> <p>The ideal scenario is to combine the high accuracy of unstructured DST with the hardware-friendliness of fine-grained structured sparsity.</p> <h2 id="dynamic-sparse-training-for-learning-structured-sparse-representations">Dynamic Sparse Training for Learning Structured Sparse Representations</h2> <p>Our work “Dynamic Sparse Training with Structured Sparsity” <d-cite key="Lasby2024SRigL"></d-cite> proposes a novel DST method called Structured RigL (SRigL) to address this challenge. SRigL aims to learn sparse networks that are both highly accurate <em>and</em> possess a structure amenable to real-world acceleration.</p> <h3 id="constant-fan-in-sparsity">Constant Fan-in Sparsity</h3> <p>SRigL modifies the RigL algorithm to enforce a <strong>constant fan-in</strong> constraint. This means each neuron (or output channel in a convolutional layer) has the same number of active incoming connections. This is a specific type of N:M sparsity (where N is the fan-in and M is the potential dense fan-in) and results in a regular structure within the weight matrices. Theoretical analysis suggests that this constant fan-in constraint should not inherently impair training dynamics and might even offer slightly better output-norm variance compared to less constrained sparsity patterns, especially for very sparse networks.</p> <h3 id="the-hidden-trick-of-dst-neuron-ablation">The Hidden Trick of DST: Neuron Ablation</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_sparsetraining_neuronablation.svg" sizes="95vw"></source> <img src="/assets/img/srigl_sparsetraining_neuronablation.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Neuron Ablation." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Neuron Ablation in DST. When a neuron has too few weight to learn useful representations, DST methods learn to ablate, or remove, the entire neuron — effectively reducing the width of the layer.</div> </div> <p>A key empirical finding was that standard unstructured DST methods, like RigL, when pushed to very high sparsity levels (&gt;90%), implicitly learn to <strong>ablate neurons</strong> — that is, it they learn to remove neurons with very few weights, effectively reducing the width of layers.</p> <p>This neuron ablation appears crucial for maintaining generalization at extreme sparsities, but enforcing a naive constant fan-in constraint would prevent this, as it would force every neuron to maintain the same number of weights, even if those weights are not useful for learning.</p> <h3 id="the-srigl-algorithm">The SRigL Algorithm</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_constantfanin.svg" sizes="95vw"></source> <img src="/assets/img/srigl_constantfanin.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Constant Fan-in Fine-grained Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: Constant Fan-in Fine-grained Sparsity. A form of N:M sparsity where every neuron on a layer is constained to have the same number of weights.</div> </div> <p>SRigL integrates the constant fan-in objective with an explicit neuron ablation mechanism. The core steps, adapted from RigL, are:</p> <ol> <li>Identify weights to prune (smallest magnitude) and potential connections to grow (largest gradient magnitude on zeroed weights).</li> <li>Count salient weights per neuron.</li> <li> <strong>Ablate neurons</strong>: If a neuron has fewer salient weights than a defined threshold ($\gamma_{sal}$ multiplied by the target fan-in), it’s entirely pruned. Its designated weights are redistributed.</li> <li>Compute the new constant fan-in based on any ablated neurons.</li> <li>Prune the globally smallest magnitude weights.</li> <li>For each <em>active</em> neuron, regrow connections to meet the target constant fan-in, prioritizing those with the largest gradient magnitudes.</li> </ol> <p>This allows SRigL to learn both fine-grained constant fan-in sparsity <em>within</em> active neurons and coarser neuron-level structured sparsity.</p> <h2 id="key-results-performance-and-acceleration">Key Results: Performance and Acceleration</h2> <p>SRigL was evaluated on image classification tasks using CIFAR-10 (ResNet-18, Wide ResNet-22) and ImageNet (ResNet-50, MobileNet-V3, ViT-B/16) <d-cite key="Lasby2024SRigL"></d-cite>.</p> <h3 id="matching-dense-accuracy-with-structured-sparsity">Matching Dense Accuracy with Structured Sparsity</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_resnet18.svg" sizes="95vw"></source> <img src="/assets/img/srigl_resnet18.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-18/CIFAR-10" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_wide_resnet22.svg" sizes="95vw"></source> <img src="/assets/img/srigl_wide_resnet22.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Wide ResNet 22/CIFAR-10" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_resnet50.svg" sizes="95vw"></source> <img src="/assets/img/srigl_resnet50.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-50/ImageNet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_vit.svg" sizes="95vw"></source> <img src="/assets/img/srigl_vit.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ViT/ImageNet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: (a) Percentage active neurons (i.e., not ablated) following RigL/SRigL training on ResNet-50/ImageNet (b) Sparse Fan-In vs. ViT layer index at the end of training with RigL at 90% sparsity.</div> </div> <p>SRigL with neuron ablation was shown to achieve generalization performance comparable to unstructured RigL and often close to the dense training baseline, even at high sparsities (e.g., 90-95%) across various architectures. Extended training further improved performance, similar to RigL.</p> <h3 id="the-importance-of-neuron-ablation">The Importance of Neuron Ablation</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_imagenet_perc_active.svg" sizes="95vw"></source> <img src="/assets/img/srigl_imagenet_perc_active.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-50/ImageNet." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_vit_rigl_fan_in.svg" sizes="95vw"></source> <img src="/assets/img/srigl_vit_rigl_fan_in.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Sparse Fan-in vs VIT layer index." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: (a) Percentage active neurons (i.e., not ablated) following RigL/SRigL training on ResNet-50/ImageNet (b) Sparse Fan-in vs. ViT layer index at the end of training with RigL at 90% sparsity.</div> </div> <p>The neuron ablation component was critical. Without it, SRigL’s performance lagged behind unstructured RigL at very high sparsities (&gt;90%) and with Vision Transformers. Enabling SRigL to ablate neurons restored performance to RigL levels. The percentage of active neurons (not ablated) learned by SRigL dynamically adapted with sparsity, mirroring RigL’s behavior. For Vision Transformers, SRigL’s performance was particularly sensitive to the ablation threshold $\gamma_{sal}$, with higher thresholds performing best, suggesting that aggressively ablating neurons to maintain sufficient density in the remaining ones is beneficial for ViTs.</p> <h3 id="real-world-speedups">Real-World Speedups</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-md-12 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_grouped-bar-threads-4.svg" sizes="95vw"></source> <img src="/assets/img/srigl_grouped-bar-threads-4.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-50/ImageNet." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-12 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/srigl_grouped-bar-threads-4-device-gpu-batch_size-2048-broken-y.svg" sizes="95vw"></source> <img src="/assets/img/srigl_grouped-bar-threads-4-device-gpu-batch_size-2048-broken-y.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Sparse Fan-in vs VIT layer index." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption">Figure: (a) Percentage active neurons (i.e., not ablated) following RigL/SRigL training on ResNet-50/ImageNet (b) Batched GPU inference with batch size of 2048 on an NVIDIA Titan V. At 90% sparsity, our condensed representation is 1.7$\times$ faster than dense and 13.0$\times$ faster than unstructured (CSR) sparse layers. Note y-axis is log-scaled..</div> </div> <p>The structured sparsity learned by SRigL (constant fan-in + ablated neurons) translates into tangible inference speedups. The paper demonstrates a “condensed” matrix multiplication method (Algorithm 1 in the paper ) that leverages this structure.</p> <ul> <li> <strong>CPU (Online Inference, single input):</strong> At 90% sparsity, SRigL’s condensed representation was up to <strong>3.4x faster than dense</strong> and 2.5x faster than unstructured (CSR) sparse layers on an Intel Xeon CPU.</li> <li> <strong>GPU (Batched Inference, batch size 256):</strong> At 90% sparsity, it was <strong>1.7x faster than dense</strong> and 13.0x faster than unstructured (CSR) sparse layers on an NVIDIA Titan V GPU.</li> </ul> <p>These speedups are achieved even with a straightforward PyTorch implementation, highlighting the practical benefits of the learned structure.</p> <h3 id="not-only-efficiency-srigl-enables-new-applications-of-neural-networks">Not Only Efficiency: SRigL Enables New Applications of Neural Networks</h3> <p>SRigL’s structured sparsity is not just about speed; it also opens up new avenues for neural networks. The ability to learn a combination of fine-grained constant fan-in and neuron-level structured sparsity enables otherwise infeasible applications with neural networks.</p> <p>One interesting use case is in <strong>extreme classification</strong>, where the number of classes can reach millions. Representing such a large number of classes with dense models is impractical due to the sheer size and complexity of the model. For instance, in a typical image classification task with 1 million classes, a dense model would require a weight matrix of size $1 \text{M} \times 1 \text{M}$, which is not only computationally expensive but also memory-intensive. Already, SRigL has been successfully applied to extreme classification in our follow-up NeurIPS 2024 work <a href="https://openreview.net/forum?id=RA6rzOJ2zI" rel="external nofollow noopener" target="_blank">“Navigating Extremes: Dynamic Sparsity in Large Output Spaces”</a> in collaboration with Aalto University and the University of Bath <d-cite key="Ullah2024NavigatingExtremes"></d-cite>.</p> <p>The same problem applies to other tasks like natural language processing (NLP) and recommendation systems, where the number of classes can be extremely large, and SRigL’s ability to learn structured sparsity can help in efficiently representing and processing these large output spaces.</p> <h2 id="conclusion-and-future-horizons">Conclusion and Future Horizons</h2> <p>“Dynamic Sparse Training with Structured Sparsity” <d-cite key="Lasby2024SRigL"></d-cite> makes a significant stride towards practical sparse neural networks. SRigL demonstrates that it’s possible to:</p> <ul> <li>Train networks from a sparse initialization to a sparse solution (sparse-to-sparse).</li> <li>Achieve generalization performance on par with state-of-the-art <em>unstructured</em> sparse training methods.</li> <li>Learn a combination of fine-grained constant fan-in and neuron-level structured sparsity.</li> <li>Realize significant real-world inference acceleration on both CPUs and GPUs due to this learned structure.</li> </ul> <p>The insight that successful DST methods at high sparsity inherently learn to reduce model width (neuron ablation) is key and SRigL formalizes this. This work underscores that much of the progress in deep learning comes from methods that better leverage hardware capabilities.</p> <ul> <li>Improving the convergence speed of DST methods, which can take longer to train than dense models.</li> <li>Exploring the potential of DST to learn novel, efficient architectures for new data domains beyond typical NLP/CV tasks, particularly in areas like “AI for Science.”</li> </ul> <p>SRigL paves the way for deploying highly efficient and accurate sparse models in a wider range of applications, making powerful AI more accessible and sustainable.</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">"Dynamic Sparse Training with Structured Sparsity" (<a href="https://t.co/MCxVCeMYt0" rel="external nofollow noopener" target="_blank">https://t.co/MCxVCeMYt0</a>) was accepted at ICLR 2024! DST methods learn state-of-the-art sparse masks, but accelerating DNNs with unstructured masks is difficult. SRigL learns structured masks, improving real-world CPU/GPU timings <a href="https://t.co/zZASJlXtRi" rel="external nofollow noopener" target="_blank">pic.twitter.com/zZASJlXtRi</a></p>— Mike Lasby (@mikelasby) <a href="https://twitter.com/mikelasby/status/1749622255339094128?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">January 23, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-17-training-sparse-structured-nn.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container territorial-acknowledgement pb-2"> We would like to acknowledge the traditional territories of the People of the Treaty 7 region in Southern Alberta which includes the Blackfoot Confederacy (comprising the Siksika, Piikani, and Kainai First Nations), as well as the Tsuut’ina First Nation, and the Stoney Nakoda (including the Chiniki, Bearspaw, and Wesley First Nations). The City of Calgary is also home to Métis Nation of Alberta, Region 3. </div> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/calgaryml/calgaryml.github.io" rel="external nofollow noopener" target="_blank">customized theme</a> based on <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos © Yani Ioannou. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T1LVR8501M"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T1LVR8501M");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>
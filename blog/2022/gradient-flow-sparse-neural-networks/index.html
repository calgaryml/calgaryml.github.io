<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient Flow in Sparse Neural Networks &amp; Why Lottery Tickets Win | Calgary Machine Learning Lab </title> <meta name="author" content=" "> <meta name="description" content="An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training."> <meta name="keywords" content="Calgary, ucalgary, University of Calgary, Alberta, u of c, machine learning, ml, neural networks, NN, deep learning, artificial intelligence, AI, lab, research lab, research group, yani ioannou, schulich, engineering"> <meta property="og:site_name" content="Calgary Machine Learning Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Calgary Machine Learning Lab | Gradient Flow in Sparse Neural Networks &amp; Why Lottery Tickets Win"> <meta property="og:url" content="https://www.calgaryml.com/blog/2022/gradient-flow-sparse-neural-networks/"> <meta property="og:description" content="An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training."> <meta property="og:image" content="/assets/img/ucmllogo-og.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Gradient Flow in Sparse Neural Networks &amp; Why Lottery Tickets Win"> <meta name="twitter:description" content="An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training."> <meta name="twitter:image" content="/assets/img/ucmllogo-og.png"> <meta name="twitter:site" content="@UCalgaryML"> <meta name="twitter:creator" content="@UCalgaryML"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": " "
        },
        "url": "https://www.calgaryml.com/blog/2022/gradient-flow-sparse-neural-networks/",
        "@type": "BlogPosting",
        "description": "An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training.",
        "headline": "Gradient Flow in Sparse Neural Networks & Why Lottery Tickets Win",
        
        "sameAs": ["https://orcid.org/0000-0002-9797-5888", "https://scholar.google.com/citations?user=Qy9yv44AAAAJ", "https://github.com/calgaryml", "https://twitter.com/UCalgaryML"],
        
        "name": " ",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://use.typekit.net/pzd6qvr.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.calgaryml.com/blog/2022/gradient-flow-sparse-neural-networks/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Gradient Flow in Sparse Neural Networks & Why Lottery Tickets Win",
            "description": "An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training.",
            "published": "February 24, 2022",
            "authors": [
              
              {
                "author": "Utku Evci",
                "authorURL": "https://www.utkuevci.com",
                "affiliations": [
                  {
                    "name": "Google",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yani Ioannou",
                "authorURL": "https://yani.ai/",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Cem Keskin",
                "authorURL": "https://scholar.google.com/citations?user=9HoiYnYAAAAJ&hl=en",
                "affiliations": [
                  {
                    "name": "Meta",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yann Dauphin",
                "authorURL": "https://www.dauphin.io",
                "affiliations": [
                  {
                    "name": "Google",
                    "url": ""
                  }
                ]
              }
              
            ],
            "doi": "10.1609/aaai.v36i6.20611",
            "url": "https://arxiv.org/abs/2010.03533",
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img class="navbar" src="/assets/img/ucmllogo-text.svg" alt="Calgary Machine Learning Lab"> </a> <div class="navbar-brand social"> <a href="mailto:%79%61%6E%69.%69%6F%61%6E%6E%6F%75@%75%63%61%6C%67%61%72%79.%63%61" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9797-5888" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Qy9yv44AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/calgaryml" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://x.com/UCalgaryML" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/labmembers/">people </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/funding/">funding </a> </li> <li class="nav-item "> <a class="nav-link" href="/places/">calgary </a> </li> <li class="nav-item "> <a class="nav-link" href="/contactus/">contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Gradient Flow in Sparse Neural Networks &amp; Why Lottery Tickets Win</h1> <p>An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#tldr">TL;DR</a></li> <li class="toc-entry toc-h2"><a href="#the-quest-for-efficient-yet-powerful-neural-networks">The Quest for Efficient Yet Powerful Neural Networks</a></li> <li class="toc-entry toc-h2"><a href="#the-importance-of-gradient-flow">The Importance of Gradient Flow</a></li> <li class="toc-entry toc-h2"> <a href="#problem-1-off-to-a-bad-start--poor-gradient-flow-at-initialization">Problem 1: Off to a Bad Start — Poor Gradient Flow at Initialization</a> <ul> <li class="toc-entry toc-h3"><a href="#dense-initialization-and-sparse-fan-in">Dense Initialization and Sparse Fan-in</a></li> <li class="toc-entry toc-h3"><a href="#a-sparsity-aware-initialization-to-fix-gradient-flow-at-initialization">A Sparsity-Aware Initialization to Fix Gradient Flow at Initialization</a></li> <li class="toc-entry toc-h3"><a href="#signal-propagation-at-initialization-with-different-initializations">Signal Propagation at Initialization with Different Initializations</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#problem-2-slogging-through--poor-gradient-flow-during-training">Problem 2: Slogging Through — Poor Gradient Flow During Training</a></li> <li class="toc-entry toc-h2"> <a href="#the-curious-case-of-lottery-tickets-lts">The Curious Case of Lottery Tickets (LTs)</a> <ul> <li class="toc-entry toc-h3"><a href="#evidence-for-lts-re-learning-the-pruning-solution">Evidence for LTs Re-learning the Pruning Solution</a></li> <li class="toc-entry toc-h3"><a href="#the-lottery-ticket-initialization-a-nudge-in-the-right-direction">The Lottery Ticket Initialization: A Nudge in the Right Direction</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#key-insights-summarized">Key Insights Summarized</a></li> <li class="toc-entry toc-h2"><a href="#conclusion-and-future-directions">Conclusion and Future Directions</a></li> <li class="toc-entry toc-h2"><a href="#citing-our-work">Citing our work</a></li> </ul> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <p>Training sparse neural networks directly from a random initialization is notoriously difficult, often resulting in poor performance compared to their dense counterparts. The paper “Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win” <d-cite key="Evci2022GradientFlow"></d-cite>, accepted for an <a href="https://aaai-2022.virtualchair.net/poster_aaai3082" rel="external nofollow noopener" target="_blank">Oral Presentation at AAAI 2022</a>, investigates this through the perspective of gradient flow and finds:</p> <ul> <li> <strong>Poor Gradient Flow at Initialization:</strong> Standard initialization techniques, designed for dense networks, are ill-suited for sparse networks due to their heterogeneous connectivity. This leads to vanishing gradients right from the start. We propose a sparsity-aware initialization that can alleviate this.</li> <li> <strong>Poor Gradient Flow During Training:</strong> Even if initialized better, sparse networks can suffer from weak gradient flow throughout training. Dynamic Sparse Training (DST) methods, which adapt network connectivity during training, can significantly improve this.</li> <li> <strong>Lottery Tickets Re-learn, Don’t Magically Fix Flow:</strong> The success of Lottery Tickets (LTs) isn’t due to them inherently having better gradient flow. Instead, LTs (which use specific initial weights from a pre-trained dense model’s history) effectively “re-learn” the good solution found by pruning the original dense model. They are guided to a known good basin of attraction, rather than finding a new one through superior optimization dynamics in a sparse setting.</li> </ul> <h2 id="the-quest-for-efficient-yet-powerful-neural-networks">The Quest for Efficient Yet Powerful Neural Networks</h2> <p>Deep Neural Networks (DNNs) are the powerhouses behind many AI breakthroughs. However, their increasing size and computational appetite pose significant challenges for deployment and training sustainability. One promising avenue for efficiency is <strong>sparsity</strong>: using networks with far fewer connections (and thus parameters) than typical dense networks.</p> <p>A common way to obtain a sparse network is by <strong>pruning</strong> a large, trained dense network. This often yields sparse models that retain the performance of the original dense model with a fraction of the parameters.</p> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/sparsetrainingproblem.svg" alt="Training Outcomes: Pruning pipeline vs. sparse training problem." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 1: (Left) The standard pruning pipeline: train a dense model, prune it, and optionally fine-tune to get a good sparse model. (Right) The sparse training problem: initializing a sparse network randomly and training it often leads to poor performance compared to the pruned model.</div> </div> <p>However, what if we want to train a sparse network from the get-go, without the costly pre-training of a dense model? This is where things get tricky. Naively initializing a network with a sparse structure and training it from scratch (the “sparse training problem”) usually leads to significantly worse performance. This begs the question: why is training sparse networks so hard, and what can we learn from the exceptions?</p> <h2 id="the-importance-of-gradient-flow">The Importance of Gradient Flow</h2> <p>Many advancements in training dense DNNs have come from understanding and improving <strong>gradient flow</strong> – how the error signals propagate backward through the network to update the weights. Poor gradient flow can lead to vanishing or exploding gradients, making training stall or become unstable. This paper <d-cite key="Evci2022GradientFlow"></d-cite> applies this lens to sparse neural networks.</p> <h2 id="problem-1-off-to-a-bad-start--poor-gradient-flow-at-initialization">Problem 1: Off to a Bad Start — Poor Gradient Flow at Initialization</h2> <h3 id="dense-initialization-and-sparse-fan-in">Dense Initialization and Sparse Fan-in</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/dense_fanin.svg" alt="Dense fan-in in a neural network." class="img-fluid rounded z-depth-0" loading="eager"> <div class="caption">(a) Dense layer, every neuron has the same number of incoming connections</div> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/sparse_fanin.svg" alt="Heterogeneous sparse fan-in in a neural network." class="img-fluid rounded z-depth-0" loading="eager"> <div class="caption">(b) Sparse layer, every neuron can have a different number of incoming connections</div> </div> </div> <div class="caption">Figure: (a) In a dense layer, each neuron has the same fan-in, (b) However, in a general unstructured sparse layer, the fan-in can vary significantly from neuron to neuron — we propose an initialization that accounts for this.</div> </div> <p>Standard weight initialization methods like Glorot/He <d-cite key="Glorot2010Understanding,He2015Delving"></d-cite> are designed with dense networks in mind. They assume that all neurons in a layer have the same number of incoming (fan-in) and outgoing (fan-out) connections.</p> <p>For example, for a layer using a ReLU activation function, the weights are initialized from a distribution with variance inversely proportional to the largest number of incoming connections (fan-in):</p> \[w_{ij}^{[l]} \sim \mathcal{N}(0, \frac{2}{\text{fan-in}}),\] <p>where $\textbf{fan-in}$ is the number of incoming connections for the layer, and $w_{ij}^{[l]}$ is the weight connecting neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$. This ensures that the variance of the output of the layer is roughly equal to the variance of its input, which helps maintain a healthy signal flow through the network.</p> <h3 id="a-sparsity-aware-initialization-to-fix-gradient-flow-at-initialization">A Sparsity-Aware Initialization to Fix Gradient Flow at Initialization</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/sparse_fanin_unequalout.svg" alt="Sparse neural network with dense init." class="img-fluid rounded z-depth-0" loading="eager"> <div class="caption">(a) Dense initialization assumes every neuron has same number of connections, and on average, uses weights that are too small</div> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/sparse_fanin_equalout.svg" alt="Sparse neural network with sparse init.." class="img-fluid rounded z-depth-0" loading="eager"> <div class="caption">(b) Sparse initialization calculates the correct weight variance for each neuron based on the number of incoming connections</div> </div> </div> <div class="caption">Figure: (a) Using a dense initialization for a sparse layer causes vanishing gradients as neurons with few connections are initialized incorrectly, however in (b) the sparse initialization accounts for the fact that fan-in can vary significantly from neuron to neuron — ensuring better behaved gradients at initialization.</div> </div> <p>In a sparse neural network, the assumption that every neuron has the same number of connections breaks down. Rather, the number of connections per neuron can be highly variable. Using dense initializations directly in sparse networks often causes the signal to vanish rapidly as it propagates through the layers. This is because neurons with fewer incoming connections are initialized with weights that are too small, leading to a weak signal. This can cause the gradients to vanish, especially in deeper networks, making it hard for the model to learn effectively.</p> <p>Our work <d-cite key="Evci2022GradientFlow"></d-cite> proposes a <strong>sparsity-aware initialization</strong> that adjusts the variance of the initial weights for each neuron based on its <em>actual</em> fan-in within the sparse structure, for example for a layer with a ReLU activation function:</p> \[w_{ij}^{[l]} \sim \mathcal{N}(0, \frac{2}{\text{fan-in}_i^{[l]}}),\] <p>where $\text{fan-in}_i^{[l]}$ is the number of incoming connections for neuron $i$ in layer $l$.</p> <h3 id="signal-propagation-at-initialization-with-different-initializations">Signal Propagation at Initialization with Different Initializations</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/sparseinit_signalprop.svg" alt="Signal Propagation at Initialization: Graph showing standard deviation of output vs. sparsity." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 3: Standard deviation of the pre-softmax output ($\sigma(f(x))$) in LeNet-5 vs. sparsity level. Dense initialization (blue) shows signal vanishing with increasing sparsity. Sparsity-aware initializations (Liu et al. <d-cite key="Liu2019Rethinking"></d-cite> and "Ours" - the paper's proposal) maintain signal strength.</div> </div> <p>This sparsity-aware initialization leads to better signal propagation at the start of training and can improve the final generalization performance, especially for networks without normalization layers like BatchNorm (e.g., LeNet5, VGG16). For models with BatchNorm (e.g., ResNet-50), the effect of initialization is less pronounced, as BatchNorm itself helps regulate signal propagation.</p> <h2 id="problem-2-slogging-through--poor-gradient-flow-during-training">Problem 2: Slogging Through — Poor Gradient Flow During Training</h2> <p>While a good initialization helps, it’s not the whole story. Sparse networks can still suffer from poor gradient flow <em>during</em> the training process.</p> <div class="container"> <div class="row justify-content-center align-items-center bg-white"> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/mnist_gradnorm_logx.svg" alt="Gradient Norm During Training: Graphs for LeNet-5, VGG-16, and ResNet-50." class="img-fluid rounded z-depth-0" loading="eager"> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/vgg_gradnorm.svg" alt="Gradient Norm During Training: Graphs for LeNet-5, VGG-16, and ResNet-50." class="img-fluid rounded z-depth-0" loading="eager"> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/assets/img/gradientflow/resnet_gradnorm.svg" alt="Gradient Norm During Training: Graphs for LeNet-5, VGG-16, and ResNet-50." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 4: Gradient norm during training for LeNet-5 (left), VGG-16 (center), and ResNet-50 (right) under different setups. 'Scratch' (training a sparse mask from random dense initialization) often shows very low gradient norm initially. 'Scratch+' (with sparsity-aware initialization) improves this. 'RigL+' (a DST method with sparsity-aware init) often shows stronger gradient flow.</div> </div> <p>This is where <strong>Dynamic Sparse Training (DST)</strong> methods come in. DST techniques, like RigL <d-cite key="Evci2020RigL"></d-cite>, don’t keep the sparse connectivity fixed. Instead, they periodically update the mask during training:</p> <ol> <li> <strong>Prune:</strong> Remove connections that have become less salient (e.g., small magnitude weights).</li> <li> <strong>Grow:</strong> Add new connections, often by identifying those that would have the largest gradient if they were active.</li> </ol> <p>The paper shows that DST methods, particularly RigL, significantly improve gradient flow during training compared to training with a fixed sparse mask. These updates can introduce new directions for optimization (e.g., by creating new negative eigenvalues in the Hessian), helping the network escape poor regions of the loss landscape. This improved gradient flow correlates with better generalization performance.</p> <h2 id="the-curious-case-of-lottery-tickets-lts">The Curious Case of Lottery Tickets (LTs)</h2> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/lotterticketsolution.svg" alt="Lottery Ticket Hypothesis Concept: Diagram illustrating the LTH process." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 5: The Lottery Ticket Hypothesis: A dense network is trained (obtaining a dense solution), then pruned. The "winning ticket" uses the *initial weights* ($\Theta_{t=0}$ or an early snapshot $\Theta_{0&lt;t \ll T}$) corresponding to the pruned mask and is then trained.</div> </div> <p>The Lottery Ticket Hypothesis (LTH) <d-cite key="Frankle2019LTH"></d-cite> proposed that within a large, randomly initialized dense network, there exist smaller subnetworks (the “winning tickets”). If these winning tickets are trained in isolation from their <em>original initialization weights</em> (or weights from very early in the dense model’s training, known as “late rewinding”), they can achieve accuracy comparable to the full dense network.</p> <p>This was exciting because it suggested a way to find highly sparse, trainable networks. However, the paper <d-cite key="Evci2022GradientFlow"></d-cite> finds something intriguing: <strong>Lottery Tickets also exhibit poor gradient flow, similar to naively trained sparse networks!</strong> (see Figure 4).</p> <p>So, if LTs don’t fix the gradient flow problem, why do they work so well? The paper’s central argument is that LTs succeed because they essentially <strong>re-learn the pruning solution</strong> they were derived from.</p> <h3 id="evidence-for-lts-re-learning-the-pruning-solution">Evidence for LTs Re-learning the Pruning Solution</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-9 mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/mnist_mds.svg" alt="MDS Plot of Solutions: 2D projection of solution distances for LeNet5." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 6: A 2D MDS projection showing the relative distances between different solutions for LeNet5. 'Lottery-start' is closer to 'Prune-end' than 'Scratch-start'. 'Lottery-end' converges very close to 'Prune-end', while 'Scratch-end' solutions are more dispersed and further away.</div> </div> <ol> <li> <strong>Proximity in Weight Space:</strong> LT initializations (the specific weight values rewound from early in dense training) start much closer in L2 distance to the final <em>pruned solution</em> (the weights of the dense model after pruning) than a random “scratch” initialization using the same mask. After training, the LT solution ends up significantly closer to this pruned solution.</li> </ol> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-6 mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/mnist_interpol.svg" alt="Loss Interpolation: Graph showing training loss along interpolation paths for LeNet5." class="img-fluid rounded z-depth-0" loading="eager"> </div> <div class="col-6 mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/resnet_interpol.svg" alt="Loss Interpolation: Graph showing training loss along interpolation paths for LeNet5." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 7: Training loss along a linear interpolation path between a starting point ($\alpha=0$, e.g., Lottery-start or Scratch-start) and the Pruned Solution ($\alpha=1$) for LeNet5. The path between 'Lottery End' and 'Pruned Solution' is relatively flat, indicating they are in the same basin. The path from 'Scratch End' often shows a barrier.</div> </div> <ol start="2"> <li> <strong>Same Basin of Attraction:</strong> By interpolating between the LT solution/initialization and the pruned solution, the paper shows that they lie within the same low-loss basin of attraction. In contrast, scratch solutions often have a high loss barrier separating them from the pruned solution’s basin.</li> </ol> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table 1: Ensemble &amp; Prediction Disagreement. We compare the function disagreement <d-cite key="Fort2019deepensembles"></d-cite> with the original pruning solution and ensemble generalization over 5 sparse ResNet50 models, trained from random initializations and LTs (lottery tickets) on ImageNet. As a baseline, we also show results for 5 pruned models trained from different random initializations. * we compare 4 different pruned models with the pruning solution LT are derived from. </div> <table> <thead> <tr> <th style="text-align: left"><strong>Initialization</strong></th> <th><strong>(Top-1) Test Accuracy</strong></th> <th><strong>Ensemble</strong></th> <th><strong>Disagreement</strong></th> <th><strong>Disagreement w/ Pruned</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">LT</td> <td>75.73 ± 0.08</td> <td>76.27</td> <td>0.0894 ± 0.0009</td> <td>0.0941 ± 0.0009</td> </tr> <tr> <td style="text-align: left">Scratch</td> <td>71.16 ± 0.13</td> <td>74.05</td> <td>0.2039 ± 0.0013</td> <td>0.2033 ± 0.0012</td> </tr> <tr> <td style="text-align: left">Pruned Soln.</td> <td>75.60</td> <td>–</td> <td>–</td> <td>–</td> </tr> <tr> <td style="text-align: left">5 Diff. Pruned</td> <td>75.65 ± 0.13</td> <td>77.80</td> <td>0.1620 ± 0.0008</td> <td>0.1623 ± 0.0011*</td> </tr> </tbody> </table> </div> <ol start="3"> <li> <strong>Functional Similarity:</strong> LT solutions are not only close in weight space but also learn very similar functions to the pruned solution they originated from. This is measured by low “disagreement” (fraction of test images classified differently) between the LT solution and the pruned solution. Ensembles of LTs derived from the same pruning process show little performance gain, further suggesting they converge to nearly identical functions.</li> </ol> <h3 id="the-lottery-ticket-initialization-a-nudge-in-the-right-direction">The Lottery Ticket Initialization: A Nudge in the Right Direction</h3> <div class="container l-screen"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/assets/img/gradientflow/initializations_explained.svg" alt="Loss Landscape Intuition: Diagram illustrating basins of attraction for LT and Scratch." class="img-fluid rounded z-depth-0" loading="eager"> </div> </div> <div class="caption">Figure 8: An intuitive illustration. A Lottery Ticket initialization (blue circle) is already positioned within the basin of attraction of the good Pruning Solution (green circle). Random (Scratch) initializations (red circles) are more likely to fall into different, potentially suboptimal, basins.</div> </div> <p><strong>The implication is powerful:</strong> LTs aren’t discovering new, highly effective sparse configurations through superior optimization dynamics. Instead, their specific initialization “nudges” the optimization process to rediscover a known good solution – the one found by pruning the dense network.</p> <h2 id="key-insights-summarized">Key Insights Summarized</h2> <p>This investigation into gradient flow in sparse neural networks reveals:</p> <ol> <li> <strong>Sparsity-Aware Initialization Matters:</strong> Naive use of dense initializations harms sparse networks by causing poor gradient flow from the start. Using initializations that account for the actual sparse connectivity is crucial.</li> <li> <strong>Dynamic Sparse Training Boosts Gradient Flow:</strong> DST methods improve gradient flow <em>during</em> training by adapting the network’s sparse connections, leading to better generalization than training with fixed sparse masks.</li> <li> <strong>Lottery Tickets are “Echoes” of Pruning:</strong> LTs work well not because they inherently possess better gradient flow, but because their specific initial weights guide them to re-learn the solution of the pruned dense model they originated from. This limits their ability to find truly novel solutions in the sparse regime.</li> </ol> <h2 id="conclusion-and-future-directions">Conclusion and Future Directions</h2> <p>Understanding gradient flow provides valuable insights into the challenges of training sparse neural networks. While sparsity-aware initializations and Dynamic Sparse Training offer promising avenues for improving how we train sparse models from scratch, the success of Lottery Tickets seems more about “remembering” a good solution than fundamentally solving the optimization difficulties in sparse landscapes.</p> <p>The journey towards efficiently training sparse neural networks that are as performant as their dense counterparts, without relying on dense pre-training or specific “winning ticket” initializations, continues. Methods that can robustly navigate the complex loss landscapes of sparse models and maintain healthy gradient flow are key to unlocking the full potential of sparse training of neural networks.</p> <h2 id="citing-our-work">Citing our work</h2> <p>If you find this work useful, please consider citing it using the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">evci2022gradientflowsparse</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Evci, Utku and Ioannou, Yani A. and Keskin, Cem and Dauphin, Yann}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 36th AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">venue</span> <span class="p">=</span> <span class="s">{Vancouver, BC, Canada}</span><span class="p">,</span>
  <span class="na">eventdate</span> <span class="p">=</span> <span class="s">{2022-02-22/2022-03-1}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">arxivid</span> <span class="p">=</span> <span class="s">{2010.03533}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2010.03533}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v36i6.20611}</span>
<span class="p">}</span>
</code></pre></div></div> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Join us for our <a href="https://twitter.com/hashtag/AAAI2022?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#AAAI2022</a> *Oral Presentation* of our paper on Sparse DNN training: "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win" tomorrow (Feb 24) @ 10:30-11:45am PST!<br>Poster at 8:45-10:30am/Feb 27 4:45-6:30pm. Work with <a href="https://twitter.com/utkuevci?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@utkuevci</a>, <a href="https://twitter.com/ynd?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@ynd</a>, <a href="https://twitter.com/cem_keskin_?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@cem_keskin_</a>. <a href="https://t.co/BZLiRqDDzZ" rel="external nofollow noopener" target="_blank">pic.twitter.com/BZLiRqDDzZ</a></p>— Yani Ioannou (@yanii) <a href="https://twitter.com/yanii/status/1496532190682927107?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">February 23, 2022</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-19-gradient-flow-sparse-neural-networks.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container territorial-acknowledgement pb-2"> We would like to acknowledge the traditional territories of the People of the Treaty 7 region in Southern Alberta which includes the Blackfoot Confederacy (comprising the Siksika, Piikani, and Kainai First Nations), as well as the Tsuut’ina First Nation, and the Stoney Nakoda (including the Chiniki, Bearspaw, and Wesley First Nations). The City of Calgary is also home to Métis Nation of Alberta, Region 3. </div> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/calgaryml/calgaryml.github.io" rel="external nofollow noopener" target="_blank">customized theme</a> based on <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos © Yani Ioannou. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T1LVR8501M"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T1LVR8501M");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>
---
layout: post
date: 2026-01-26 00:00:00-0700
inline: true
---

[Mike Lasby's](/labmembers/) collaborative work with Cerebras, "REAP the experts: Why pruning prevails for one-shot moe compression" {% cite lasby2025reapexpertspruningprevails %}, has been accepted at the [International Conference on Learning Representations (ICLR), 2026](https://iclr.cc/Conferences/2026).
This work explores the compression of Mixture of Experts (MoE) models through pruning techniques, demonstrating that REAP (Random Expert and Prune) outperforms existing expert-merging focused methods in terms of efficiency and performance retention.

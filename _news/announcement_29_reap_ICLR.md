---
layout: post
date: 2026-01-26 00:00:00-0700
inline: true
---

[Mike Lasby's](/labmembers/) collaborative work with Cerebras, "REAP the experts: Why pruning prevails for one-shot moe compression" {% cite lasby2025reapexpertspruningprevails %}, has been accepted at the [International Conference on Learning Representations (ICLR), 2026](https://iclr.cc/Conferences/2026).
This work explores the compression of Sparse Mixture of Experts (SMoE) models through expert compression techniques, demonstrating that REAP (Router-weighted Expert Activation Pruning) outperforms existing expert merging and pruning methods in terms of compressed model quality retention.

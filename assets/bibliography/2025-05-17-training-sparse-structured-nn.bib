@inproceedings{Lasby2024SRigL,
  title={{Dynamic Sparse Training with Structured Sparsity}},
  author={Mike Lasby and Anna Golubeva and Utku Evci and Mihai Nica and Yani A. Ioannou},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={[https://openreview.net/forum?id=rMv2Tw5q50](https://openreview.net/forum?id=rMv2Tw5q50)},
  note={\href{[https://arxiv.org/abs/2305.02299](https://arxiv.org/abs/2305.02299)}{arXiv:2305.02299}[cite: 137]. Presentation [cite: 1, 12, 29, 46]}
}

@article{Evci2020RigL,
  title={{Rigging the Lottery: Making All Tickets Winners}},
  author={Utku Evci and Trevor Gale and Jacob Menick and Pablo Samuel Castro and Erich Elsen},
  journal={arXiv preprint arXiv:1911.11134},
  year={2020},
  note={Published in International Conference on Machine Learning (ICML) 2020. Presentation [cite: 20, 23]}
}

@article{Mocanu2018SET,
  title={{Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science}},
  author={Decebal Constantin Mocanu and Elena Mocanu and Peter Stone and Phuong H. Nguyen and Madeleine Gibescu and Antonio Liotta},
  journal={Nature Communications},
  volume={9},
  number={1},
  pages={2383},
  year={2018},
  note={Presentation [cite: 19, 22]}
}

@inproceedings{Frankle2019LTH,
  title={{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}},
  author={Jonathan Frankle and Michael Carbin},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  note={Presentation [cite: 63, 66]}
}

@inproceedings{Frankle2020LinearMode,
  title={{Linear Mode Connectivity and the Lottery Ticket Hypothesis}},
  author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},
  year={2020},
  note={Presentation [cite: 70, 74, 75, 90, 102, 107, 113]}
}

@inproceedings{Evci2022GradientFlow,
  title={{Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win}},
  author={Utku Evci and Yani A. Ioannou and Cem Keskin and Yann Dauphin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6577--6586},
  year={2022},
  note={Presentation[cite: 4, 5, 83, 88, 89, 125]. Also arXiv:2010.03533}
}

@article{Liu2019RethinkingPruning,
  author    = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  title     = {Rethinking the Value of Network Pruning},
  journal   = {arXiv preprint arXiv:1810.05270},
  year      = {2018},
  note      = {Published in International Conference on Learning Representations (ICLR) 2019. Presentation [cite: 69, 78]}
}

@article{Mishra2021Accelerating,
  title={{Accelerating Sparse Deep Neural Networks}},
  author={Asit Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021},
  note={Paper [cite: 151, 210, 218]}
}

@techreport{Nvidia2020Ampere,
  author      = {{Nvidia}},
  title       = {{Nvidia A100 Tensor Core GPU Architecture}},
  institution = {Nvidia},
  year        = {2020},
  note        = {Paper [cite: 210]}
}

@article{Gale2020StateOfSparsity,
  title={The State of Sparsity in Deep Neural Networks},
  author={Trevor Gale and Erich Elsen and Sara Hooker},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019},
  note={Published in Proceedings of Machine Learning and Systems 2020. Presentation [cite: 79]}
}

@article{Ramanujan2020WhatsHidden,
  title={Whatâ€™s Hidden in a Randomly Weighted Neural Network?},
  author={Vivek Ramanujan and Mitchell Wortsman and Aniruddha Kembhavi and Ali Farhadi and Mohammad Rastegari},
  journal={arXiv preprint arXiv:1911.13299},
  year={2019},
  note={Published in CVPR 2020. Presentation [cite: 80]}
}

@inproceedings{Jain2024WinningTickets,
 author    = {Rohan Jain and Mohammed Adnan and Ekansh Sharma and Yani Ioannou},
 title     = {Winning Tickets from Random Initialization: Aligning Masks for Sparse Training},
 booktitle = {2nd Workshop on Unifying Representations in Neural Models (UniReps), NeurIPS 2024 Workshops},
 year      = {2024},
 note      = {Presentation [cite: 2]}
}

@inproceedings{Ullah2024NavigatingExtremes,
 author    = {Nasib Ullah and Erik Schultheis and Mike Lasby and Yani Ioannou and Rohit Babbar},
 title     = {Navigating Extremes: Dynamic Sparsity in Large Output Spaces},
 booktitle = {38th Annual Conference Neural Information Processing Systems (NeurIPS) 2024},
 year      = {2024},
 note      = {Presentation [cite: 9, 10]}
}
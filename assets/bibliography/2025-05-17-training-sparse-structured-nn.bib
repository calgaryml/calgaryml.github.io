@inproceedings{Lasby2024SRigL,
  title = {Dynamic Sparse Training with Structured Sparsity},
  author = {Mike Lasby and Anna Golubeva and Utku Evci and Mihai Nica and Yani A. Ioannou},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024},
  eprint = {2305.02299},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2305.02299}
}

@inproceedings{Evci2020RigL,
  title = {Rigging The Lottery: Making All Tickets Winners},
  author = {Utku Evci and Trevor Gale and Jacob Menick and Pablo Samuel Castro Rivadeneira and Erich Elsen},
  year = {2020},
  booktitle = {International Conference of Machine Learning (ICML)},
  eprint = {1911.11134},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1911.11134}
}

@article{Mocanu2018SET,
  title = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author = {Decebal Constantin Mocanu and Elena Mocanu and Peter Stone and Phuong H. Nguyen and Madeleine Gibescu and Antonio Liotta},
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {2383},
  year = {2018},
  publisher = {Nature Publishing Group},
  eprint = {1707.04780},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi = {10.1038/s41467-018-04316-3}
}

@inproceedings{Frankle2019LTH,
  title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author = {Jonathan Frankle and Michael Carbin},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2019},
  eprint = {1803.03635},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1803.03635}
}

@inproceedings{Frankle2020LinearMode,
  title = {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  author = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  year = {2020},
  eprint = {1912.05671},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1912.05671}
}

@inproceedings{Evci2022GradientFlow,
  title={Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win},
  author={Utku Evci and Yani A. Ioannou and Cem Keskin and Yann Dauphin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  volume={36},
  number={6},
  pages={6577--6586},
  year={2022},
  doi={10.1609/aaai.v36i6.20611},
  eprint = {2010.03533},
  archivePrefix = {arXiv},
  eprinttype = {arxiv}
}

@inproceedings{Liu2019RethinkingPruning,
  author    = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  title     = {Rethinking the Value of Network Pruning},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2019},
  eprint    = {1810.05270},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi       = {10.48550/arXiv.1810.05270}
}

@article{Mishra2021Accelerating,
  title = {Accelerating Sparse Deep Neural Networks},
  author = {Asit Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
  journal = {arXiv preprint arXiv:2104.08378},
  year = {2021},
  doi = {10.48550/arXiv.2104.08378},
  eprint = {2104.08378},
  archivePrefix = {arXiv},
  eprinttype = {arxiv}
}

@techreport{Nvidia2020Ampere,
  author      = {Nvidia},
  title       = {Nvidia A100 Tensor Core GPU Architecture},
  institution = {Nvidia},
  year        = {2020}
}

@article{Gale2020StateOfSparsity,
  title = {The State of Sparsity in Deep Neural Networks},
  author = {Trevor Gale and Erich Elsen and Sara Hooker},
  journal = {arXiv preprint arXiv:1902.09574},
  year = {2019},
  eprint = {1902.09574},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1902.09574}
}

@inproceedings{Ullah2024NavigatingExtremes,
 author    = {Nasib Ullah and Erik Schultheis and Mike Lasby and Yani Ioannou and Rohit Babbar},
 title     = {Navigating Extremes: Dynamic Sparsity in Large Output Spaces},
 booktitle = {38th Annual Conference Neural Information Processing Systems (NeurIPS) 2024},
 year      = {2024},
  eprint    = {2411.03171},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  doi       = {10.48550/arXiv.2411.03171}
}